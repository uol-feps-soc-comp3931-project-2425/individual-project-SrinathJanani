Progress Notes :
================


29/1/2025 Notes

Code finally works using Ohollo's Chord Detection
• python versions 2.7, 3.6, 3.9 and 3.10 did not work
-> python version 3.8 finally worked
• vamp continuously threw errors during installment
-> vamp had to be manually downloaded and configured to path
• the library threw errors when pip installing
-> the library had to be installed using the GitHub url
• there were multiple complications with other required libraries and library versions (example, numpy)
-> these issues somehow resolved automatically after completing the above fixes
• other chord detection libraries offered similar issues and bugs
• often libraries or required components didn't have helpful enough documentation

After much deliberation and testing, the chord detection library has successfully been installed for python3.8

Implemented Features :
• defining the extractor object (courtesy of Chordino)
• extracting chords and their respective timestamps from locally stored songs (one at a time)
• storing the above features from a 'ChordChange' object instead into arrays
• splicing the storage location to obtain the song name and artist

Upcoming Features :
• designing an object to store
  - song name
  - music artist
  - key signature
  - chord progression (universal notation) with timestamps (dictionary/array)
  - (optional - release date)
• creating a naming convention for song files, which must include
  - song name
  - music artist
  - key signature (to make universal chord progression notation)
• looping over extracting from multiple songs (say, in a folder)
• verifying detection accuracy (currently speculated at roughly 80%)

Future Implementations :
• compiling database of songs (possible to use top X number of songs on spotify for each year over a duration of time)
• researching on machine learning technique to classify and regress data
• installing and importing machine learning models
• training the model against the data, and subsequently testing for similarities with new songs

Additional Possibilities :
• designing an app to interact with data and display outputs
• designing a method to record audio, from which chords can be extracted

__________

30/1/2025

Note : other potential chord extraction softwares that were considered
https://github.com/orchidas/Chord-Recognition
https://github.com/belovm96/chord-detection/tree/master?tab=readme-ov-file#Usage
https://pypi.org/project/chorder/
https://pypi.org/project/autochord/#description

An issue with depending on externally provided key signature is that this will also need to be provided when testing with audio, and this results in excess effort when making a custom recording or if the song's scale is unknown to the user.
Hence it is more optimal to have a key signature identification algorithm to detect the scale.

__________

31/1/2025

The following method aims to calculate the key signature of a song, given the chord extraction output.
This method is inspired from "https://www.oajaiml.com/uploads/archivepdf/74301105.pdf" (Section 3.2, page 77)

The key signature can be estimated by taking all extracted notes and comparing them to all possible scales, and select the scale with the most number of chords detected part of that scale.
Due to the rarity of chords appearing outside of the scale especially in pop songs, this method should hypothetically have high accuracy.

Simplifying the algorithm :

• A problem arises with variations of a chord from the same starting note (example,  "F" "F#" "F#5" "F#7" "F#m" "F#m7" "F#sus4" "F2" "F5" "F6" "F7" "F7sus4" "Fm" "Fm13" "Fm7" "Fmaj" "Fmaj7")
-> These can be stripped down to just the essential chords, i.e F and Fm (starting from F), and F# and F#m (starting from F#). This castly reduces the number of computations required.

• Of all the scales available, the main 2 "major" and "minor" scales will be considered. This means for 12 notes and 2 variations of each note, there are 24 total possible key signatures.
-> In music theory, there is a relation between minor and major scales where the third note of a minor scale forms a major scale using the exact same chords. This reduces the number of scales to just 12. 

• Each note will have to be verified among every scale. But a convenient trick to use would be to represent all possible scales by their note as a binary sequence. In the sequence, each digit corresponds to whether the chord is present in that scale.
-> Hence [ A, A#/Bb, B, C, ... , G, G# ] will be converted to a 12 digit binary sequence.

• Instead of checking every single scale, it is notable that the 1st 4th and 5th chords are major, the 2nd 3rd and 6th chords are minor, and the 7th chord can either be considered both major or minor.
i.e given a major chord for example, we would only need to find the scales where that chord is the 1st chord (C major scale), the 4th chord (G major scale), the 5th chord (F major scale), or possibly the 7th chord (C# major scale).
-> Hence any chord will only appear in 1 of 4 scales. The above example of C major chord would result in binary output 000110001010

• Moving a scale by i notes, each note in the scale also moves by i positions.
-> this means that a chord 'i' notes ahead will (cyclic) shift right by i positions
for example, the d major chord, which is 2 positions ahead of c major, will be represented by 100001100010 i.e A major (4th chord), D major (1st chord), D# major (plausible 7th chord) and G major (5th chord)
-> this additionally implies that only 2 binary sequences are required to be stored (1 for major, 1 for minor), and cyclic shift operations can be performed on it to indicate which scales it is part of.
-> this technique also accounts for both major and minor chords of a given note, readily being able to use either sequence depending on whether it is a major or minor chord.

->> Hence given an extracted chord, the first letter is selected. If the second position is "#" or "m", it is also extracted. If the second position was "#", if the third position is "m" then it is also selected. 
Then based on the distance from A, the respective binary sequence is selected and cyclic shift can be performed on it.
These binary vectors will be added together for each extracted chord, and the position with the highest count is highly probable to be the key signature of the song.

Pseudo-code
>>> key_signature_array = [0,0,0,0,0,0,0,0,0,0,0,0]
>>> for chord in extraction_array:
>>> position_diff = ['A', 'A#', 'B', 'C', ... , 'G', 'G#'].index(chord)
>>> sequence = [ [1,1,0,0,0,1,0,1,0,0,0,0] , [0,1,0,1,0,0,0,0,1,0,1,0] ].index(any(x=="m" for x in chord))
>>> scale_array = cylic_shift(sequence, position_diff)
>>> key_signature_array += scale_array
>>> key_signature = max(key_signature_array)

__________


3/2/25

Note :
Detailed explanation of roman numerical chord (triad) representation is given in
https://www.musictheory.net/lessons/44
https://www.musictheory.net/lessons

The current prototype of the pre-solution obtains a good estimate for the key signature when the extracted chords have good accuracy.
Given extracted chords and a key signature, the machine learning model will need to access chord progressions not as the actual notes being, but as the relative distance from the starting note.
This is notationally represented as I (root note), II, III, IV, V, VI, and VII

Since different songs use different scales, the universal way to compare different chord progressions is to use the roman numerals system.
Hence an algorithm is required which converts notes to their respective roman numeral representation, depending on the key signature of the song.

This can be achieved by cyclicly shifting the previously used array for storing the scale notes i.e ['A', 'A#', ... , 'G#'] to start with the root note of the scale, and comparing the notes to the index of this array, they can be converted via the same position of an array ['I', 'bII', 'II', 'bIII', ... , 'bVII', 'VII']
This conversion can hence be stored in a seperate array, which will be used for the training data.

Note: The useful music theory required for this section is
• Each chord is a "triad", a set of 3 notes seperated by a note each
• The chord is labelled with a roman numeral by the numerical position of the first note of that chord
-> i.e the chord from the first note is labelled as I, the chord from the second note is labelled as II, etc
• Major chords are represented in capital (i.e II, VI) and minor chords are represented in lower case (i.e ii, vi)
• A 'Flat' (b) represents that the note is played exactly one position below the written note next to it (i.e Ab is played as the note to the left of A)
• A 'Sharp' (#) represents that the note is played exactly one position above the written note next to it (i.e A# is played as the note to the right of A)
• In case a chord is not part of the scale (since the scale only uses 7 of the total 12 notes), then it is written with a sharp or flat to the left of the chord (i.e bIII, #iv)
• Though there is much more musical notation, this study shall limit the notational use to this amount (to make it easier for classication)

__________


15/2/25

Choice of data collection:

The current project progress has been successful in being able to extract chords from a song file to a rough accuracy. 
My plan is to find the top 10 artists globally via spotify, and select their 10 top songs (again via spotify) to try and unbias my data while retaining a pop genre in my dataset.

The issue now arises, that for a fixed dataset like described above, more accuracy will lead to a more accurate research project. In this case, having a 100% accuracy of the data would be preferred. 
This can be firmly achieved by manually constructing the dataset, or also by webscraping the data from music websites. The former is very tedious, and the latter may have ethical issues.

Although I currently possess a method of extracting chords, it would be beneficial to try new methods to create the dataset. Despite this, the code for this chord extraction will still be useful for the testing phase of the project

__________

9/3/25

With the basic code ready to process the data, the current task is the collection of the dataset.
After extensive research on existing databases and methods, the initial plan was to build a webscraper to extract the chords from music websites. This however has potential legal and ethical issues associated with it. In the end, the very creation of the web scraper was too expensive (time wise) to be feasible. 

This research of existing databases and implementations of similar projects also gave clarity onto the structure of the dataset. It is as follows:
dataset = [
["artist1", "song 1", 'key_signature', [ [ChordProgression1], [ChordProgression2], ... ]],
["artist1", "song2", 'key_signature', [ [ChordProgression1] ]], #songs might have only 1 unique chord progression
...
["artistX", "songY", 'key_signature', [ [ChordProgression1], [ChordProgression2], ... ]]
]
Here, each array of a chord progression will represent sections of the song where 4 chords dominantly represent the musical structure of the song. Hence each array will have length 4, consisting of chords in that progression.

Through this research, it has been found that there are no similar databases or projects that endeavour towards the same goal. This implies that there are no ready databases that can be used. The closest Thus for this project, I will require to construct the database on my own. The other problem with webscraping the raw data is that the effort to process the extracted data will be immense, making it as intensive as manually curating the database.

Through all this research, I have determined that it is best to manually construct the required database. Though the manual effort will take a considerable amount of time, it is more time-efficient that other methods, and will ensure high quality control in the data for this project.

The most helpful websites to identify the correct chords have been
• Hook Theory (Most accurate, with comprehensive info, but can be reductive and omit chords)
• Chordify (Also accurate with sufficient chord information, with limited live chord overviews per day)
• Ultimate Guitar (widely variable accuracy due to information being user generated)
• E-Chords (concise formatting of information, unreliable accuracy)

Stupid things key signature
One of the girls - weeknd
marry you - Bruno mars


------------------

17th March 2025

The database is almost complete. The findings so far are that chord information is not very accurate on the internet, with HookTheory having the most accurate and comprehensive information which has been crowd-sourced. However, some songs on HookTheory are missing sections of new chords, since the website stops analysing at the first instance of a part of the song (i.e first verse, first chorus, etc).
Chordify instead automatically transcribes chords from audio. This is not always accurate, but covers all chords with sufficient accuracy, while missing information like the key signature and labelling parts of the song.
If both these websites failed, then using another website would suffice. It is important to note that in the creation of the database, all songs' chords were manually tested and ensured to be accurate using my guitar and https://muted.io/piano-chords/

With each chord progression being a 4-d array to be enumerated into a 4-d vector, the data must account for repeating artists for the same data points, as well as a non-linear relation withing datapoints of artists. The best fit of a ML model has been chosen as k-Nearest Neighbour, and similarity to all artists can be calculated as the Euclidian distance. 

To enumerate the chords, they will be ranked by their usage, source:
https://www.hooktheory.com/blog/chord-progression-search-patterns-and-trends/ AND
https://www.hooktheory.com/trends#key=Rel&scale=major

From this website, the % use of chords has been found to be:
I - 13% + 1%
i - _ + 14%
bII - _ + 0.7%
bii - 
II - 
ii - 3% + 0.3%
bIII - 0.4%
biii - 
III - _ + 6%
iii - 3% + 0.2%
IV - 10% + 1%
iv - 0.9% + 5%
bV - _ + 0.2%
bv - 0.2%
V - 9% + 3%
v - 0.2% + 3%
bVI - 0.7%
bvi - 
VI - _ + 9%
vi - 7% + 0.2%
bVII - 1%
bvii - _ + 0.2%
VII - _ + 9%
vii - _ + 0.2%


--------------

24/3/25 

The database and related functions for pre-processing the data. 
The goal of the project will now be to conduct various types of ML algorithm testing. This will test over:
- kNN for different numbers of neighbours
- binary classification iterated over all artists
- linear regression
- decision trees, random forest?
- support vector machine?


Various factors affecting the output
- songs are not always made by the main artist
- artists use a variety of chord progressions and chords, some common occurences is not universal among all artists
- the style of a musician is also seen in the instruments, production, type of vocal performance, etc. Hence the chords alone might not provide a clear insight onto musical style.
- the algorithms used only attempt to analyse the musical data, rather than provide a solution of describing musical styles
- this model and data is a small-scale prototype, since the level of detail of the data is intricate
- collaborations of multiple artists may mix different styles together in the music

-------------------

2/4/25

The total variations of testing can be attributed to:
- the ordering of enumeration of the chords (2)
- using the lone enumeration vs obtaining the difference to relate both successive chords (2)
- the type of machine learning applied (5 + dummy)
- number of artists being considered at a time
- splitting of training and testing data


------------------------

Literature Review 


Chord prediction:
- https://medium.com/@huanlui/chordsuggester-i-3a1261d4ea9e
- https://www.irjmets.com/uploadedfiles/paper//issue_12_december_2023/46945/final/fin_irjmets1701708528.pdf
- *** https://arxiv.org/html/2410.22046v1#Sx4
- https://github.com/gabrantes/ML-Chord-Progressions

>> Genre classification:
- https://ionides.github.io/students/cpyang_honors_thesis.pdf
- https://www.oajaiml.com/uploads/archivepdf/74301105.pdf
- https://www.mage.ai/blog/music-genre-classification
- https://medium.com/@joshuadavidgottlieb/classifying-music-genres-based-on-song-lyrics-dc380a281954 (by lyric)
- https://towardsdatascience.com/classifying-song-genre-using-spotifys-built-in-features-vs-extracting-my-own-a4d5fe448948/
- https://www.oajaiml.com/uploads/archivepdf/74301105.pdf
- https://www.researchgate.net/publication/319569608_Using_simplified_chords_sequences_to_classify_songs_genres (by simplified chord sequences)
- https://ieeexplore.ieee.org/document/8019531
- https://arxiv.org/abs/1902.03283

>> Chord recognition:
- https://medium.com/@locopornatureza/exploring-the-intersection-of-deep-learning-and-music-my-journey-in-creating-a-chord-recognition-9d2f3b1c1be8
- https://onlinelibrary.wiley.com/doi/10.1155/2021/5590996
- https://www.mdpi.com/2076-3417/6/5/157

>> Major vs Minor classification:
- https://www.kaggle.com/code/ahmetcelik158/mathematics-of-music-chord-classification#4.-Model-Building

Chord Generation:
- https://cs229.stanford.edu/proj2015/136_report.pdf
- https://asmp-eurasipjournals.springeropen.com/articles/10.1186/s13636-023-00288-5
- https://ismir2023program.ismir.net/lbd_316.html
- https://www.mdpi.com/2227-7390/11/5/1111

Instrument classification:
- https://www.tandfonline.com/doi/abs/10.1076/jnmr.32.1.3.16798

<< Hit song prediction:
- https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1154663/full

classifying by song features:
- https://digibuo.uniovi.es/dspace/bitstream/handle/10651/49411/Chord.pdf?sequence=1&isAllowed=y

>>>> classifying artists:
- https://arxiv.org/abs/1901.04555
- https://github.com/jcguidry/classical-music-artist-classification
- https://researchportal.northumbria.ac.uk/en/publications/a-multi-task-music-artist-classification-network
- https://www.ee.columbia.edu/~dpwe/pubs/aes02-aclass.pdf
- https://www.ifs.tuwien.ac.at/~knees/publications/knees_oegai05-1.pdf
- https://www.researchgate.net/publication/2486763_Artist_Detection_in_Music_with_Minnowmatch














